{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lactate Discordance Project\n",
    "## Classifier Development\n",
    "### C.V. Cosgriff, MIT Critical Data\n",
    "\n",
    "In this notebook we develop two gradient boosting models to classifiy lactate discordance in the bottom and top quartiles of the APACHE IVa distribution. We create these separetely as the EDA suggested difference in the nature of lactate discordance across groups. We are not applying any missingness procedures because we will use `xgBoost` which is rather robust to missing data and can learn from the missingness. \n",
    "\n",
    "The notebook will be structured as follows:\n",
    "* Data preparation and subcohort creation\n",
    "* Low APACHE IVa cohort classifier grid search\n",
    "* High APACHE IVa cohort classifier grid search\n",
    "\n",
    "\n",
    "As stated above, we'll use the extreme gradient boosting algorithim to fit our models. Hyperparameters will be obtained by a random sampling of the hyperparaemter space per Bergstra et al. who showed this to be superior than grid searching _(Journal of Machine Learning Research 13 (2012) 281-30)_ and estimating the generalization error via 10-fold cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Envrionment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import sem\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "cohort = pd.read_csv('./cleaned_cohort.csv')\n",
    "\n",
    "# \"Tableau 20\" colors as RGB.   \n",
    "tableau20 = [(31, 119, 180), (174, 199, 232), (255, 127, 14), (255, 187, 120),    \n",
    "             (44, 160, 44), (152, 223, 138), (214, 39, 40), (255, 152, 150),    \n",
    "             (148, 103, 189), (197, 176, 213), (140, 86, 75), (196, 156, 148),    \n",
    "             (227, 119, 194), (247, 182, 210), (127, 127, 127), (199, 199, 199),    \n",
    "             (188, 189, 34), (219, 219, 141), (23, 190, 207), (158, 218, 229)]  \n",
    "  \n",
    "# Scale the RGB values to the [0, 1] range, which is the format matplotlib accepts.    \n",
    "for i in range(len(tableau20)):    \n",
    "    r, g, b = tableau20[i]    \n",
    "    tableau20[i] = (r / 255., g / 255., b / 255.)\n",
    "\n",
    "marker = ['v','o','d','^','s','>','+']\n",
    "ls = ['-','-','-','-','-','s','--','--']\n",
    "\n",
    "# configure matplotlib\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "plt.style.use('classic')\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "\n",
    "# configure jupyter for using matplotlib\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Prepare Cohort Data for Modeling\n",
    "\n",
    "Because we are using boosted trees, we do not need to scale or normalize. We do however need to convert any categorical variables; this applies to ethnicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra index we don't need from csv\n",
    "cohort = cohort.drop('Unnamed: 0', axis = 1)\n",
    "\n",
    "# rename ethnicity labels to make them like variable names\n",
    "cohort.loc[cohort.ethnicity == 'African American', 'ethnicity'] = 'african_american'\n",
    "cohort.loc[cohort.ethnicity == 'Native American', 'ethnicity'] = 'native_american'\n",
    "cohort.loc[cohort.ethnicity == 'Other/Unknown', 'ethnicity'] = 'other'\n",
    "cohort.loc[cohort.ethnicity == 'Asian', 'ethnicity'] = 'asian'\n",
    "cohort.loc[cohort.ethnicity == 'Caucasian', 'ethnicity'] = 'caucasian'\n",
    "cohort.loc[cohort.ethnicity == 'Hispanic', 'ethnicity'] = 'hispanic'\n",
    "eth = pd.get_dummies(cohort.ethnicity, prefix = 'eth')\n",
    "eth = eth.drop('eth_caucasian', axis = 1)\n",
    "cohort = pd.concat([cohort, eth], axis = 1)\n",
    "cohort = cohort.drop('ethnicity', axis = 1)\n",
    "\n",
    "# set index to stay ID\n",
    "cohort = cohort.set_index('patientunitstayid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define subcohorts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_apache_cohort = cohort.loc[cohort.apache_quartile == cohort.apache_quartile.min()]\n",
    "high_apache_cohort = cohort.loc[cohort.apache_quartile == cohort.apache_quartile.max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check check the $n$ in each cohort and how many of them had the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low APACHE IVa cohort contains 13204 patients with 11.50% have a discordant lactate level.\n",
      "High APACHE IVa cohort contains 12244 patients with 30.45% have a discordant lactate level.\n"
     ]
    }
   ],
   "source": [
    "print('Low APACHE IVa cohort contains {} patients with {:.2f}% have a discordant lactate level.'\\\n",
    "      .format(low_apache_cohort.shape[0], low_apache_cohort.lactate_discordance.mean()*100))\n",
    "print('High APACHE IVa cohort contains {} patients with {:.2f}% have a discordant lactate level.'\\\n",
    "      .format(high_apache_cohort.shape[0], high_apache_cohort.lactate_discordance.mean()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now turn to modeling the low APACHE IVa cohort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Low APACHE IVa Classifier\n",
    "\n",
    "We start by dropping lactate and APACHE information from the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_apache_cohort = low_apache_cohort.drop(['lactate_min', 'lactate_max', 'apache_quartile', 'apachescore'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we form design and target label matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_label = low_apache_cohort.lactate_discordance.astype('int32')\n",
    "low_apache_cohort = low_apache_cohort.drop('lactate_discordance', axis = 1)\n",
    "design_matrix = low_apache_cohort.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we form a test/train split. We'll save 25% of the data for final testing of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(design_matrix, target_label, test_size = 0.25, random_state = 42)\n",
    "\n",
    "# save the data and split\n",
    "low_apache_cohort.to_csv('./la_cohort.csv')\n",
    "np.save('X_train_la.npy', X_train)\n",
    "np.save('y_train_la.npy', y_train)\n",
    "np.save('X_test_la.npy', X_test)\n",
    "np.save('y_test_la.npy', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the parameters for grid search. Some notes on this:\n",
    "* Max depth usually ends up as 6, 7, or 8\n",
    "* The learning rate is generally around 0.05 and shouldn't get above 0.3\n",
    "* Minimum child weight, subsample proportion, and column sample by tree proportion can be used to adjust overfitting\n",
    "* The number of trees (estimators) can be turned up quite high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'objective':['binary:logistic'],\n",
    "          'learning_rate': [0.01, 0.05, 0.1],\n",
    "          'max_depth': [3, 6, 9, 12],\n",
    "          'min_child_weight': [6, 8, 10, 12],\n",
    "          'silent': [True],\n",
    "          'subsample': [0.6, 0.8, 1],\n",
    "          'colsample_bytree': [0.5, 0.75, 1],\n",
    "          'n_estimators': [500, 750, 1000]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we perform a random grid search with 5-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1000 candidates, totalling 5000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=64)]: Done  72 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=64)]: Done 322 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=64)]: Done 672 tasks      | elapsed: 10.5min\n",
      "[Parallel(n_jobs=64)]: Done 1122 tasks      | elapsed: 17.1min\n",
      "[Parallel(n_jobs=64)]: Done 1672 tasks      | elapsed: 24.7min\n",
      "[Parallel(n_jobs=64)]: Done 2322 tasks      | elapsed: 34.5min\n",
      "[Parallel(n_jobs=64)]: Done 3072 tasks      | elapsed: 44.7min\n",
      "[Parallel(n_jobs=64)]: Done 3922 tasks      | elapsed: 57.3min\n",
      "[Parallel(n_jobs=64)]: Done 4872 tasks      | elapsed: 71.7min\n",
      "[Parallel(n_jobs=64)]: Done 5000 out of 5000 | elapsed: 73.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=<generator object _BaseKFold.split at 0x7f9bd6004fc0>,\n",
       "          error_score='raise',\n",
       "          estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1),\n",
       "          fit_params=None, iid=True, n_iter=1000, n_jobs=64,\n",
       "          param_distributions={'objective': ['binary:logistic'], 'learning_rate': [0.01, 0.05, 0.1], 'min_child_weight': [6, 8, 10, 12], 'max_depth': [3, 6, 9, 12], 'n_estimators': [500, 750, 1000], 'silent': [True], 'subsample': [0.6, 0.8, 1], 'colsample_bytree': [0.5, 0.75, 1]},\n",
       "          pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 5\n",
    "model_xgb = XGBClassifier()\n",
    "skf = StratifiedKFold(n_splits = K, shuffle = True, random_state = 42)\n",
    "cv_grid_search_la = RandomizedSearchCV(model_xgb, param_distributions = params, n_iter = 1000, scoring = 'roc_auc',\\\n",
    "                                    n_jobs = 64, cv = skf.split(X_train, y_train), verbose = 1, random_state = 42 )\n",
    "cv_grid_search_la.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then examine the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=0.5, gamma=0, learning_rate=0.01, max_delta_step=0,\n",
      "       max_depth=6, min_child_weight=6, missing=None, n_estimators=1000,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=0.6)\n",
      "\n",
      "The CV estimated AUC for this model was 0.781.\n"
     ]
    }
   ],
   "source": [
    "print('Final Model')\n",
    "print(cv_grid_search_la.best_estimator_)\n",
    "print('\\nThe CV estimated AUC for this model was {0:.3f}.'.format(cv_grid_search_la.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: High APACHE IVa Classifier\n",
    "\n",
    "Again we start by dropping lactate and APACHE information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_apache_cohort = high_apache_cohort.drop(['lactate_min', 'lactate_max', 'apache_quartile', 'apachescore'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then form the design matrix and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_label = high_apache_cohort.lactate_discordance.astype('int32')\n",
    "high_apache_cohort = high_apache_cohort.drop('lactate_discordance', axis = 1)\n",
    "design_matrix = high_apache_cohort.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we form a test/train split. We'll again save 25% of the data for final testing of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(design_matrix, target_label, test_size = 0.25, random_state = 42)\n",
    "\n",
    "# save the data and split\n",
    "high_apache_cohort.to_csv('./ha_cohort.csv')\n",
    "np.save('X_train_ha.npy', X_train)\n",
    "np.save('y_train_ha.npy', y_train)\n",
    "np.save('X_test_ha.npy', X_test)\n",
    "np.save('y_test_ha.npy', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using the same random grid search approach, and we'll use the same hyperparameter search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'objective':['binary:logistic'],\n",
    "          'learning_rate': [0.01, 0.05, 0.1],\n",
    "          'max_depth': [3, 6, 9, 12],\n",
    "          'min_child_weight': [6, 8, 10, 12],\n",
    "          'silent': [True],\n",
    "          'subsample': [0.6, 0.8, 1],\n",
    "          'colsample_bytree': [0.5, 0.75, 1],\n",
    "          'n_estimators': [500, 750, 1000]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we perform a random grid search with 5-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1000 candidates, totalling 5000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=64)]: Done  72 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=64)]: Done 322 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=64)]: Done 672 tasks      | elapsed: 10.8min\n",
      "[Parallel(n_jobs=64)]: Done 1122 tasks      | elapsed: 17.6min\n",
      "[Parallel(n_jobs=64)]: Done 1672 tasks      | elapsed: 25.5min\n",
      "[Parallel(n_jobs=64)]: Done 2322 tasks      | elapsed: 35.6min\n",
      "[Parallel(n_jobs=64)]: Done 3072 tasks      | elapsed: 46.1min\n",
      "[Parallel(n_jobs=64)]: Done 3922 tasks      | elapsed: 58.8min\n",
      "[Parallel(n_jobs=64)]: Done 4872 tasks      | elapsed: 73.6min\n",
      "[Parallel(n_jobs=64)]: Done 5000 out of 5000 | elapsed: 75.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=<generator object _BaseKFold.split at 0x7f9bd6022410>,\n",
       "          error_score='raise',\n",
       "          estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1),\n",
       "          fit_params=None, iid=True, n_iter=1000, n_jobs=64,\n",
       "          param_distributions={'objective': ['binary:logistic'], 'learning_rate': [0.01, 0.05, 0.1], 'min_child_weight': [6, 8, 10, 12], 'max_depth': [3, 6, 9, 12], 'n_estimators': [500, 750, 1000], 'silent': [True], 'subsample': [0.6, 0.8, 1], 'colsample_bytree': [0.5, 0.75, 1]},\n",
       "          pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 5\n",
    "model_xgb = XGBClassifier()\n",
    "skf = StratifiedKFold(n_splits = K, shuffle = True, random_state = 42)\n",
    "cv_grid_search_ha = RandomizedSearchCV(model_xgb, param_distributions = params, n_iter = 1000, scoring = 'roc_auc',\\\n",
    "                                    n_jobs = 64, cv = skf.split(X_train, y_train), verbose = 1, random_state = 42 )\n",
    "cv_grid_search_ha.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.05, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=10, missing=None, n_estimators=500,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=0.6)\n",
      "\n",
      "The CV estimated AUC for this model was 0.812.\n"
     ]
    }
   ],
   "source": [
    "print('Final Model')\n",
    "print(cv_grid_search_ha.best_estimator_)\n",
    "print('\\nThe CV estimated AUC for this model was {0:.3f}.'.format(cv_grid_search_ha.best_score_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
