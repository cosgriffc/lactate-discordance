{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lactate Discordance Project\n",
    "## Cohort Extraction\n",
    "### C.V. Cosgriff, MIT Critcial Data\n",
    "\n",
    "The goal of this preliminary work is to extract vital signs, lab data, and diagnosis data in an attempt to model what we have termed _lactate discordance_. We define this as patients who have a lactate level that is discordant with their severity of illness. This definition will be further defined other notebooks. The stages of the extraction are as follows:\n",
    "\n",
    "1. Build materialized views for first day vitals, labs, vasopressor and ventilation status.\n",
    "    * Of note, vitals and labs have built sanity checks for the values.\n",
    "    * The cohort has flags for certain exclusion criteria.\n",
    "2. For diagnosis groups, a prior exploratory analysis was carried out in R, and that code was utilized to generate an R script which outputs a csv file with the first day diagnosis groups. Briefly this script:\n",
    "    * Uses regular expressions to identify eICU diagnosis strings corresponding to different diagnostic groupings weakly based on the Elixhauser AHRQ groupings.\n",
    "    * The strings are then used to bin the first day diagnoses of all ICU patients.\n",
    "    * Final a CSV file is output.\n",
    "3. We'll then grab the APACHE result tabl in order to attain APACHE IVa scores.\n",
    "4. After the views are built we'll load them into a `DataFrame` and then merge them.\n",
    "5. Finally, we'll pull in the patients table, exclude patients under a certain age and with stays that are too short and merge it with all of the other data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "\n",
    "# postgres envrionment setup; placeholds here, place your own info\n",
    "sqluser = 'mimicuser'\n",
    "userpass = 'harvardmit2018'\n",
    "dbname = 'eicu'\n",
    "schema_name = 'eicu_crd'\n",
    "host = '10.8.0.1'\n",
    "\n",
    "query_schema = 'SET search_path TO ' + schema_name + ';'\n",
    "\n",
    "# connect to the database\n",
    "con = psycopg2.connect(dbname = dbname, user = sqluser, host = host, password = userpass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Materialized Views & Diagnosis Group Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The views we will generate are as follows:\n",
    "1. Vitals, first day (`vitalsfirstday`)\n",
    "2. Labs, first day (`labsfirstday`)\n",
    "3. Vasoressors, first day (`pressorfirstday`)\n",
    "4. Ventilation, first day (`ventfirstday`)\n",
    "\n",
    "Each of these scripts was written in SQL and is based off similar MIMIC-III scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query_safely(sql, con):\n",
    "    cur = con.cursor()\n",
    "    try:\n",
    "        cur.execute(sql)\n",
    "    except:\n",
    "        # if an exception, rollback, rethrow the exception - finally closes the connection\n",
    "        cur.execute('rollback;')\n",
    "        raise\n",
    "    finally:\n",
    "        cur.close()\n",
    "    return\n",
    "\n",
    "def generate_materialized_view(query_file, query_schema):\n",
    "    with open(query_file) as fp:\n",
    "        query = ''.join(fp.readlines())\n",
    "    print('Generating materialized view using {} ...'.format(query_file), end = ' ')\n",
    "    execute_query_safely(query_schema + query, con)\n",
    "    print('done.')\n",
    "    \n",
    "generate_materialized_view('./sql/vitalsfirstday.sql', query_schema)\n",
    "generate_materialized_view('./sql/labsfirstday.sql', query_schema)\n",
    "generate_materialized_view('./sql/pressorfirstday.sql', query_schema)\n",
    "generate_materialized_view('./sql/ventfirstday.sql', query_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the diagnosis groups, we use the R script, `diagnosis_groups.R`, which is the local directory in the `R/` folder. It requires that you have `RPostgresSQL`, and the `tidyverse` packages. And of course you need R installed. \n",
    "\n",
    "_Note: make sure to edit the R script to work with your database environment._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!Rscript ./R/diagnosis_groups.R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That concludes scripts we need to run to generate the data. We can next load these scripts into dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Construct Cohort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by generating a query to pull in the base cohort, and the data from the materialized views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = query_schema + '''\n",
    "WITH apache AS (\n",
    "    SELECT patientunitstayid, apachescore\n",
    "    FROM apachepatientresult\n",
    "    WHERE apacheversion = 'IVa'\n",
    "    )\n",
    "    \n",
    "SELECT p.patientunitstayid, p.age, p.ethnicity\n",
    "    , CASE  \n",
    "        WHEN p.gender = 'Male' THEN 1\n",
    "        ELSE 0\n",
    "        END AS male_gender\n",
    "        , vs.HR_Min\n",
    "        , vs.HR_Max\n",
    "        , vs.HR_Mean\n",
    "        , vs.SBP_periodic_Min\n",
    "        , vs.SBP_periodic_Max\n",
    "        , vs.SBP_periodic_Mean\n",
    "        , vs.DBP_periodic_Min\n",
    "        , vs.DBP_periodic_Max\n",
    "        , vs.DBP_periodic_Mean\n",
    "        , vs.MAP_periodic_Min\n",
    "        , vs.MAP_periodic_Max\n",
    "        , vs.MAP_periodic_Mean\n",
    "        , vs.SBP_aperiodic_Min\n",
    "        , vs.SBP_aperiodic_Max\n",
    "        , vs.SBP_aperiodic_Mean\n",
    "        , vs.DBP_aperiodic_Min\n",
    "        , vs.DBP_aperiodic_Max\n",
    "        , vs.DBP_aperiodic_Mean\n",
    "        , vs.MAP_aperiodic_Min\n",
    "        , vs.MAP_aperiodic_Max\n",
    "        , vs.MAP_aperiodic_Mean\n",
    "        , vs.RR_Min\n",
    "        , vs.RR_Max\n",
    "        , vs.RR_Mean\n",
    "        , vs.SpO2_Min \n",
    "        , vs.SpO2_Max\n",
    "        , vs.SpO2_Mean \n",
    "        , vs.TempC_Min\n",
    "        , vs.TempC_Max\n",
    "        , vs.TempC_Mean\n",
    "        , la.ALBUMIN_min -- dropped anion gap\n",
    "        , la.ALBUMIN_max -- will be too colinear\n",
    "        , la.BANDS_min\n",
    "        , la.BANDS_max\n",
    "        , la.BICARBONATE_min\n",
    "        , la.BICARBONATE_max\n",
    "        , la.BILIRUBIN_min\n",
    "        , la.BILIRUBIN_max\n",
    "        , la.CREATININE_min\n",
    "        , la.CREATININE_max\n",
    "        , la.CHLORIDE_min\n",
    "        , la.CHLORIDE_max\n",
    "        , la.GLUCOSE_min\n",
    "        , la.GLUCOSE_max\n",
    "        , la.HEMATOCRIT_min\n",
    "        , la.HEMATOCRIT_max\n",
    "        , la.HEMOGLOBIN_min\n",
    "        , la.HEMOGLOBIN_max\n",
    "        , la.LACTATE_min\n",
    "        , la.LACTATE_max\n",
    "        , la.PLATELET_min\n",
    "        , la.PLATELET_max\n",
    "        , la.POTASSIUM_min\n",
    "        , la.POTASSIUM_max\n",
    "        , la.PTT_min\n",
    "        , la.PTT_max\n",
    "        , la.INR_min\n",
    "        , la.INR_max\n",
    "        , la.PT_min\n",
    "        , la.PT_max\n",
    "        , la.SODIUM_min\n",
    "        , la.SODIUM_max\n",
    "        , la.BUN_min\n",
    "        , la.BUN_max\n",
    "        , la.WBC_min\n",
    "        , la.WBC_max\n",
    "        , pr.pressor\n",
    "        , mv.oobintubday1\n",
    "        , ap.apachescore\n",
    "    , CASE\n",
    "        WHEN p.unitdischargeoffset < 240 THEN 1\n",
    "        ELSE 0\n",
    "        END AS short_los\n",
    "FROM patient p\n",
    "INNER JOIN vitalsfirstday vs\n",
    "ON p.patientunitstayid = vs.patientunitstayid\n",
    "INNER JOIN labsfirstday la\n",
    "ON p.patientunitstayid = la.patientunitstayid\n",
    "LEFT JOIN pressorfirstday pr\n",
    "ON p.patientunitstayid = pr.patientunitstayid\n",
    "INNER JOIN ventfirstday mv\n",
    "ON p.patientunitstayid = mv.patientunitstayid\n",
    "INNER JOIN apache ap\n",
    "ON p.patientunitstayid = ap.patientunitstayid;\n",
    "'''\n",
    "\n",
    "base_cohort = pd.read_sql_query(query, con)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next load in the diagnoses table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx_table = pd.read_csv('./dx-firstday_groupings.csv')\n",
    "del dx_table['Unnamed: 0'] # gets rid of firs column which is a duplicate index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then join this with the cohort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = base_cohort.merge(dx_table, on = 'patientunitstayid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Apply Exclusion and Basic Cleaning\n",
    "\n",
    "1. Exclude if any LoS flags are valued to 1\n",
    "2. Convert age from character to numeric, excluding <16, >89\n",
    "3. Exclude patients without a lactate measurement\n",
    "4. Change NA's for vasopressors to 0 since NA here just meant no infusions recorded\n",
    "\n",
    "We can print the shape at each point in order to track exclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cohort.shape)\n",
    "cohort = cohort[cohort.short_los == 0]\n",
    "print(cohort.shape)\n",
    "cohort = cohort[(cohort.age != '> 89') & (cohort.age != '')]\n",
    "print(cohort.shape)\n",
    "cohort.age = cohort.age.astype(float)\n",
    "cohort = cohort[cohort.age >= 16.]\n",
    "print(cohort.shape)\n",
    "cohort = cohort[(pd.isna(cohort.lactate_max) == False) & (pd.isnull(cohort.lactate_max) == False)]\n",
    "print(cohort.shape)\n",
    "cohort.loc[pd.isna(cohort.pressor), 'pressor'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can save this to a file for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Save File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort.to_csv('cohort.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that brings us to the analysis phase."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
